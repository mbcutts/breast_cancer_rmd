---
title: "Final Project R Code"
author: "Mitchell Cutts & Morgan Cox"
date: "May 12, 2021"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(keras)
library(neuralnet)
library(tensorflow)
suppressMessages(library( fields))
library(tictoc)
library(datasets)
library(mlbench)
library(Hmisc)
library(dplyr)
setwd("C:/Users/User1/Desktop/Spring2021/Advanced Stats")
```

#Loading Data and Preprocessing - Data Description

```{r}
data(BreastCancer)
di <- describe(BreastCancer)
head(BreastCancer)

BreastCancer[,'Class'] <- as.numeric(BreastCancer[,'Class'] == "benign")
head(BreastCancer)

BreastCancer = na.omit(BreastCancer)
head(BreastCancer)

#no missing data points, all points are numeric, however there is an ID variable which we will throw out for training.
#9 training variables
#Got data set from mlbench!
#https://search.r-project.org/CRAN/refmans/mlbench/html/BreastCancer.html
```

#Training and Testing Split (75/25)
```{r}
#consulted https://stackoverflow.com/questions/17200114/how-to-split-data-into-training-testing-sets-using-sample-function for this problem. Was looking for a built in function for R to do this but this manual method works just as well.
sample_size <- floor(nrow(BreastCancer) * 0.75)
set.seed(12345)
training_indices <- sample(seq_len(nrow(BreastCancer)), size = sample_size)
response = c("Class")

train <- BreastCancer[training_indices, ]
test <- BreastCancer[-training_indices, ]
trainX <- select (train, -Class)
trainY<- select (train, Class)
testX <- select (test, -Class)
testY<- select (test, Class)


trainXM <- data.matrix(trainX[,2:10])
testXM <- data.matrix(testX[,2:10])


trainYMC <- to_categorical(trainYM,2)
#trainYM[cbind(1:length(trainY), testY)]

dim(trainXM)
dim(trainYM)
head(trainYM)

```


#Statistical Methods and Graphical Methods -- Standard Neural Network Architecture 
```{r}
### setup object
### setup object
NN_model_1 <- keras_model_sequential()
### add network architecture
NN_model_1 %>% layer_dense(units = nrow(trainXM),  activation ='sigmoid', input_shape = c(1, ncol(trainXM))) %>%
  layer_dense(units = 2, activation ='softmax')
### compile
NN_model_1 %>% compile(loss ='categorical_crossentropy',optimizer = optimizer_adam(),metrics = c('accuracy'))

tic()
NN_model_1 %>% fit(trainXM, trainYMC, epochs=15, batch_size=32)
toc()
```

#GLM Classifier - Binomial Regression
```{r}

logit <- glm(Class ~ Cl.thickness + Cell.size + Cell.shape + Marg.adhesion + Epith.c.size + Bare.nuclei + Bl.cromatin, family = binomial(link='logit'), data=train)

preds <- predict(logit, testX, type="response")
library(ROCR)
pr = prediction(preds, test$Class)
prf = performance(pr, measure="tpr", x.measure="fpr")
plot(prf)

summary(logit)

plot(logit)
auc <- performance(pr, measure="auc")
auc@y.values[[1]]
```

```{r}
pos_ind <- which(preds > 0.5)
zero_ind <- which(preds <= 0.5)
y_hat = c()
y_hat[pos_ind] = 1
y_hat[zero_ind]= 0 
confusion<-table(testY$Class, y_hat)
print(confusion)
accuracy <- diag(confusion)/ rowSums(confusion)
print(accuracy)
```

```{r}

hist(preds)
hist(testY$Class)

```

